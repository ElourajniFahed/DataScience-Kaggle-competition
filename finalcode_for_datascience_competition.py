# -*- coding: utf-8 -*-
"""FinalCode for DataScience Competition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FetvrrVqYaKihQ6Cm56G2Yw9wtq2uDsp
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import numpy as np
import sklearn
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_log_error
from sklearn.model_selection import *
import os
import tensorflow as tf 
import random
from  sklearn.ensemble import *
# Machine Learning
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR, LinearSVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor

# Ensembling
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import VotingRegressor

# Metrics
from sklearn import preprocessing
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from xgboost import XGBRegressor

# %matplotlib inline

"""from google.colab import drive
drive.mount('/content/drive')"""

# Define the reproducibility 
os.environ['PYTHONHASHSEED']=str(1)
tf.random.set_seed(1)
np.random.seed(1)
random.seed(1)


# Read the training data
train_data=pd.read_csv("train.csv")


######################################################### Data Cleanign #########################################################

# Define a funtion that will apply an imputation for the features 
def Imputation(train_data):

  #### Personalu URL
  train_data["Personal URL"]=train_data["Personal URL"].apply(lambda x : 1 if str(x)!="nan" else 0)# change the personal url with 1 if the url exsist and 0 if nan

  #### Profile Cover Image Status
  train_data["Profile Cover Image Status"]=train_data["Profile Cover Image Status"].apply(lambda x : "Not set" if str(x)=="nan" else x)# change the nan values for Profile Cover Image Status as a Non set value

  #### Profile text color

  train_data["Profile Text Color"]=train_data["Profile Text Color"].apply(lambda x : 1 if str(x)!="nan" else 0)# change the Profile Text Color into 1 if the color exist and 0 if nan

  #### Profile page color

  train_data["Profile Page Color"]=train_data["Profile Page Color"].apply(lambda x : 1 if str(x)!="nan" else 0)# change the Profile Page Color into 1 if the color exist and 0 if nan

  #### Profile Theme Color

  train_data["Profile Theme Color"]=train_data["Profile Theme Color"].apply(lambda x : 1 if str(x)!="nan" else 0)# change the Profile Theme Color into 1 if the color exist and 0 if nan

  #### User Time Zone & UTC Offset

  ind=train_data["User Time Zone"][train_data["User Time Zone"].isnull()==True].index # get the indexe of the rows that have a null User Time Zone
  train_data["User Time Zone"].loc[ind]="UTC" # change the nan values of User Time Zone to UTC  because UTC is the most popular User Time Zone
  train_data["UTC Offset"].loc[ind]=0 # change the nan values of UTC Offset  to 0 

  #### Location

  indesxes=train_data["Location"][train_data["Location"].isnull()].index # get the index of rows that have a location with null value
  train_data["Location"].loc[indesxes]="NYC + 70 Countries Worldwide" # change the nan value of the location to NYC + 70 Countries Worldwide because it is the most generale value

  #### Avg Daily Profile Visit Duration in seconds & Avg Daily Profile Clicks

  ind=train_data["Avg Daily Profile Visit Duration in seconds"][train_data["Avg Daily Profile Visit Duration in seconds"].isnull()==True].index # get the rows that have a nan value for the Avg Daily Profile Visit Duration in seconds 
  train_data["Avg Daily Profile Visit Duration in seconds"].loc[ind]=train_data["Avg Daily Profile Visit Duration in seconds"].mean() # change the nan values of the Avg Daily Profile Visit Duration in seconds to the mean value of the Avg Daily Profile Visit Duration in seconds
  ind=train_data["Avg Daily Profile Clicks"][train_data["Avg Daily Profile Clicks"].isnull()==True].index# get the rows that have a nan value for the Avg Daily Profile Clicks
  train_data["Avg Daily Profile Clicks"].loc[ind]=train_data["Avg Daily Profile Clicks"].mean()# change the nan values of the Avg Daily Profile Clicks to the mean of Avg Daily Profile Clicks

  return train_data # return the new imputated data

train_data=Imputation(train_data)# apply the imputation function to our data

train_data=train_data.drop(['Id', 'User Name','Location'
                            
                            ],axis=1) # drop some irrelavent features such as the ID, User Name and the Location


######################################################### Feature Extraction #########################################################



# Define a feature engineering function that will apply some feature engineering on a given dataset
def Feature_Engineering(train_data,test_d=False):

  #### Profile Creation Timestamp
  train_data["Profile Creation Timestamp"]= pd.to_datetime(train_data["Profile Creation Timestamp"])# change the object Profile Creation Timestamp feature to a time feature 
  train_data["year_profile_creation"]=train_data["Profile Creation Timestamp"].dt.year # create a new feature that represent the years
  train_data["seasons_profile_creation"]=(train_data["Profile Creation Timestamp"].dt.month%12 + 3)//3 #1 winter,2 fall, 3 summer ,4 automne # create a new feature that represent the saison
  train_data=train_data.drop("Profile Creation Timestamp",axis=1)# drop the Profile Creation Timestamp feature after extracting the nformations from it

  #### Profile Verification Status

  one_hote_columns=pd.get_dummies(train_data["Profile Verification Status"])# apply a one_hote encoding on Profile Verification Status
  train_data=train_data.join(one_hote_columns).drop("Profile Verification Status",axis=1) # add the new created features and drop the Profile Verification Status feature

  #### Is Profile View Size Customized?

  train_data["Is Profile View Size Customized?"].replace([True, False], [1, 0], inplace=True)# replace the boolean values of the Is Profile View Size Customized?" feature with 0,1 values : 0 for false and 1 for true

  #### Profile Cover Image Status

  train_data["Profile Cover Image Status"].replace(["Set", "Not set"], [1, 0], inplace=True)# replace the Profile Cover Image Status feature with 0,1 values : 0 for nonset and 1 for set


  #### Location Public Visibility
  train_data["Location Public Visibility"]=train_data["Location Public Visibility"].apply(lambda x: 'disabled' if x=="??" else str.lower(x)) # change the values in lowercase and replace the ?? with disabled
  train_data["Location Public Visibility"].replace(["enabled", "disabled"], [1, 0], inplace=True)#replace the Location Public Visibility values with 0 and 1 values : 1 for enable and 0for disabled
  

  #### Num of Followers
  train_data["Num of Followers"]=np.log10(train_data["Num of Followers"]+1)# apply the log transformation to rescale the Num of Followers feature values


  #### Num of People Following
  train_data["Num of People Following"]=np.log10(train_data["Num of People Following"]+1)# apply the log transformation to rescale the Num of People Following feature values

  #### Num of Status Updates

  train_data["Num of Status Updates"]=np.log10(train_data["Num of Status Updates"]+1)# apply the log transformation to rescale the Num of Status Updates feature values

  #### Num of Direct Messages

  train_data["Num of Direct Messages"]=np.log10(train_data["Num of Direct Messages"]+1)# apply the log transformation to rescale the Num of Direct Messages feature values

  #### Profile Category

  train_data["Profile Category"]= train_data["Profile Category"].apply(lambda x : "unknown" if x ==" " else x)# change the emptyvalue of the Profile Category feature into unkown value
  one_hote_columns=pd.get_dummies(train_data["Profile Category"])# apply a one hote encoding to the Profile Category features
  train_data=train_data.join(one_hote_columns).drop("Profile Category",axis=1) #add the new created features and delete the Profile Category feature

  #### Num of Profile Likes
  if test_d==False:
    # we used if test_d==False because the Num of Profile Likes feature exist only on the train data , not in the test data
    train_data["Num of Profile Likes"]=np.log10(train_data["Num of Profile Likes"]+1) # apply a log  transformation to rescale the Num of Profile Likes values 

  #### User language
  #Apply a frequancy encoding on the User language because it has more than 20 unique value, we can not apply the one-hote encoding
  enc_nom_1 = (train_data.groupby('User Language').size()) / len(train_data)
  dictt=dict(zip(enc_nom_1.index,enc_nom_1.values))
  train_data["User Language"]=train_data["User Language"].apply(lambda x : dictt[x])

  #### User Time Zone
  #Apply a frequancy encoding on the User Time Zone because it has more than 20 unique value, we can not apply the one-hote encoding
  enc_nom_1 = (train_data.groupby('User Time Zone').size()) / len(train_data)
  dictt=dict(zip(enc_nom_1.index,enc_nom_1.values))
  train_data["User Time Zone"]=train_data["User Time Zone"].apply(lambda x : dictt[x])  

  return train_data #return the new feature engineered dataset

d=Feature_Engineering(train_data) # apply the feature engineering function on the emputated dataset

d=d.drop("Profile Image",axis=1)# drop the Profile Image feature because we are not gonna use the image in our prediction

#Define a function that wcill correct the aoutliers in a dataset giving a target feature
#for each pair of features(featre of the dataset, target feature) we will correct the outlier giving thoes two features

######################################################### Outlier correction #########################################################

def Outlier_Correction(data,column):
  for c in data.columns :
    for j in data[c].unique():


      d=data[data[c]==j][column]
      #Q1 = d.quantile(0.25)
      #Q3 = d.quantile(0.75)
      #IQR = Q3-Q1
      #outlier= d[((d<Q1-1.5*IQR)|(d>Q3+1.5*IQR))].index

      #data=data.drop(outlier,axis=0)
      upper_lim=d.quantile(0.95)# Calculate the max limite 
      lower_lim=d.quantile(0.05)#calculate the minimum limite
      d[d< lower_lim]=lower_lim #if a vlaue of the feature surpasses the uper limit then it will get the value of the upper limit
      d[d> upper_lim]=upper_lim #if a vlaue of the feature is lower then the lower limit then it will get the value of the lower  limit
      data[column].loc[d.index]=d.values #replace the outlier values in the dataset with the corresponding values
      

  return data# return the new corrected data 

d=Outlier_Correction(d,'Num of Profile Likes') #apply the correction function on the feature engineered data features with the Num of Profile Likes target value

# Prepare the data for the prediction task


######################################################### Model Creation and training  #########################################################

X=d.drop("Num of Profile Likes",axis=1)# Define the feature data set 
y=d["Num of Profile Likes"] #define the target value

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.00000000001,random_state=1)# split the data into training and validation set

# Trying to improve with the ensemble of all predictors
# Instnciate the predictive model that gived us the lowest loss values
RFRG=RandomForestRegressor(n_estimators=1000,n_jobs=-1)# instanciate the RandomForest predictive model
gb=GradientBoostingRegressor()# instanciate the Gradient Boosting regressor  predictive model
xgb=XGBRegressor()# instanciate the Xtream gradient boosting predictive model

# Bagg all the predigtive models(weak models into a one strong model)
clf_vote = VotingRegressor(
    estimators=[
        ('Random_forest', RFRG),#Random Foret
        ('XGB', xgb), #XGBOOST
        ("Gradient_boosting",gb),#Gradient Boosting

        ],
    weights=[ 1, 2, 1],
    )

clf_vote.fit(X_train, y_train) # train the strong model with the train features and the train targets

preds=clf_vote.predict(X_test)#Predict the test data set
l=mean_squared_log_error(10**y_test,10**preds) #calculate the loss of the predictive values comparing to the true test target values
#PS: we applied the 10**y because before the training phase we transformed the target value y using the log10 transformation 
#so in order to retreive the original value and not the scaled values we apply 10**y, to finally calculate the loss with respect to the 
#original target values and not the scaled ones



######################################################### Model prediction for unseen data #########################################################


test_data=pd.read_csv("test.csv")# read the test data that will be submitted to kaggle

d_test=Imputation(test_data)#apply the imputation function on the test data 
id=d_test["Id"]# get the id feature
d_test=d_test.drop(['Id', 'User Name','Location'],axis=1)# drop the irrelavent features frm the dataset

d_test=Feature_Engineering(d_test,test_d=True)# apply the feature engineering on the test data
# ps : we used test_d=True so that inside the function we will not apply the transformation on the Num of Profile Likes becaue cootrairly to the 
# training set , the test set does not have the Num of Profile Likes featurs

d_test=d_test.drop("Profile Image",axis=1) #drop the profile image 



predictions=clf_vote.predict(d_test)# use the trained model to predict the Num of Profile Likes for the test set

######################################################### DataFrame Creation #########################################################

#Create a data frame with the new predicted values and the id feature
dicct={}
dicct["Id"]=id
dicct["Predicted"]=10**(predictions)
df=pd.DataFrame(dicct)

df.to_csv("preds.csv",index=False) # save the data set into the curent folder

